@misc{akibaOptunaNextgenerationHyperparameter2019,
  title = {Optuna: {{A Next-generation Hyperparameter Optimization Framework}}},
  shorttitle = {Optuna},
  author = {Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
  year = {2019},
  month = jul,
  number = {arXiv:1907.10902},
  eprint = {1907.10902},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1907.10902},
  urldate = {2024-05-06},
  abstract = {The purpose of this study is to introduce new design-criteria for next-generation hyperparameter optimization software. The criteria we propose include (1) define-by-run API that allows users to construct the parameter search space dynamically, (2) efficient implementation of both searching and pruning strategies, and (3) easy-to-setup, versatile architecture that can be deployed for various purposes, ranging from scalable distributed computing to light-weight experiment conducted via interactive interface. In order to prove our point, we will introduce Optuna, an optimization software which is a culmination of our effort in the development of a next generation optimization software. As an optimization software designed with define-by-run principle, Optuna is particularly the first of its kind. We will present the design-techniques that became necessary in the development of the software that meets the above criteria, and demonstrate the power of our new design through experimental results and real world applications. Our software is available under the MIT license (https://github.com/pfnet/optuna/).},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/wudw/Zotero/storage/CUISFJJ5/Akiba et al. - 2019 - Optuna A Next-generation Hyperparameter Optimizat.pdf}
}

@inproceedings{chenXGBoostScalableTree2016,
  title = {{{XGBoost}}: {{A Scalable Tree Boosting System}}},
  shorttitle = {{{XGBoost}}},
  booktitle = {Proceedings of the 22nd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Chen, Tianqi and Guestrin, Carlos},
  year = {2016},
  month = aug,
  eprint = {1603.02754},
  primaryclass = {cs},
  pages = {785--794},
  doi = {10.1145/2939672.2939785},
  urldate = {2024-05-06},
  abstract = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/wudw/Zotero/storage/G7Z9DMQ9/Chen and Guestrin - 2016 - XGBoost A Scalable Tree Boosting System.pdf}
}

@article{ertlFastCalculationMolecular2000,
  title = {Fast {{Calculation}} of {{Molecular Polar Surface Area}} as a {{Sum}} of {{Fragment-Based Contributions}} and {{Its Application}} to the {{Prediction}} of {{Drug Transport Properties}}},
  author = {Ertl, Peter and Rohde, Bernhard and Selzer, Paul},
  year = {2000},
  month = oct,
  journal = {Journal of Medicinal Chemistry},
  volume = {43},
  number = {20},
  pages = {3714--3717},
  publisher = {American Chemical Society},
  issn = {0022-2623},
  doi = {10.1021/jm000942e},
  urldate = {2024-05-06},
  abstract = {Molecular polar surface area (PSA), i.e., surface belonging to polar atoms, is a descriptor that was shown to correlate well with passive molecular transport through membranes and, therefore, allows prediction of transport properties of drugs. The calculation of PSA, however, is rather time-consuming because of the necessity to generate a reasonable 3D molecular geometry and the calculation of the surface itself. A new approach for the calculation of the PSA is presented here, based on the summation of tabulated surface contributions of polar fragments. The method, termed topological PSA (TPSA), provides results which are practically identical with the 3D PSA (the correlation coefficient between 3D PSA and fragment-based TPSA for 34\,810 molecules from the World Drug Index is 0.99), while the computation speed is 2-3 orders of magnitude faster. The new methodology may, therefore, be used for fast bioavailability screening of virtual libraries having millions of molecules. This article describes the new methodology and shows the results of validation studies based on sets of published absorption data, including intestinal absorption, Caco-2 monolayer penetration, and blood-brain barrier penetration.},
  file = {/Users/wudw/Zotero/storage/DKI75HAJ/Ertl et al. - 2000 - Fast Calculation of Molecular Polar Surface Area a.pdf}
}

@misc{Falcon_PyTorch_Lightning_2019,
  title = {{{PyTorch}} Lightning},
  author = {Falcon, William and {The PyTorch Lightning team}},
  year = {2019},
  month = mar,
  doi = {10.5281/zenodo.3828935},
  copyright = {Apache-2.0}
}

@misc{galDropoutBayesianApproximation2016,
  title = {Dropout as a {{Bayesian Approximation}}: {{Representing Model Uncertainty}} in {{Deep Learning}}},
  shorttitle = {Dropout as a {{Bayesian Approximation}}},
  author = {Gal, Yarin and Ghahramani, Zoubin},
  year = {2016},
  month = oct,
  number = {arXiv:1506.02142},
  eprint = {1506.02142},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1506.02142},
  urldate = {2024-05-06},
  abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs -- extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/wudw/Zotero/storage/728JPXM3/Gal and Ghahramani - 2016 - Dropout as a Bayesian Approximation Representing .pdf}
}

@article{heidChempropMachineLearning2024,
  title = {Chemprop: {{A Machine Learning Package}} for {{Chemical Property Prediction}}},
  shorttitle = {Chemprop},
  author = {Heid, Esther and Greenman, Kevin P. and Chung, Yunsie and Li, Shih-Cheng and Graff, David E. and Vermeire, Florence H. and Wu, Haoyang and Green, William H. and McGill, Charles J.},
  year = {2024},
  month = jan,
  journal = {Journal of Chemical Information and Modeling},
  volume = {64},
  number = {1},
  pages = {9--17},
  publisher = {American Chemical Society},
  issn = {1549-9596},
  doi = {10.1021/acs.jcim.3c01250},
  urldate = {2024-05-06},
  abstract = {Deep learning has become a powerful and frequently employed tool for the prediction of molecular properties, thus creating a need for open-source and versatile software solutions that can be operated by nonexperts. Among the current approaches, directed message-passing neural networks (D-MPNNs) have proven to perform well on a variety of property prediction tasks. The software package Chemprop implements the D-MPNN architecture and offers simple, easy, and fast access to machine-learned molecular properties. Compared to its initial version, we present a multitude of new Chemprop functionalities such as the support of multimolecule properties, reactions, atom/bond-level properties, and spectra. Further, we incorporate various uncertainty quantification and calibration methods along with related metrics as well as pretraining and transfer learning workflows, improved hyperparameter optimization, and other customization options concerning loss functions or atom/bond features. We benchmark D-MPNN models trained using Chemprop with the new reaction, atom-level, and spectra functionality on a variety of property prediction data sets, including MoleculeNet and SAMPL, and observe state-of-the-art performance on the prediction of water-octanol partition coefficients, reaction barrier heights, atomic partial charges, and absorption spectra. Chemprop enables out-of-the-box training of D-MPNN models for a variety of problem settings in fast, user-friendly, and open-source software.},
  file = {/Users/wudw/Zotero/storage/6UD3HZ9X/Heid et al. - 2024 - Chemprop A Machine Learning Package for Chemical .pdf}
}

@article{labuteWidelyApplicableSet2000,
  title = {A Widely Applicable Set of Descriptors},
  author = {Labute, Paul},
  year = {2000},
  month = jan,
  journal = {Journal of Molecular Graphics and Modelling},
  volume = {18},
  number = {4},
  pages = {464--477},
  issn = {1093-3263},
  doi = {10.1016/S1093-3263(00)00068-1},
  urldate = {2024-05-06},
  abstract = {Three sets of molecular descriptors computable from connection table information are defined. These descriptors are based on atomic contributions to van der Waals surface area, log P (octanol/water), molar refractivity, and partial charge. The descriptors are applied to the construction of QSAR/QSPR models for boiling point, vapor pressure, free energy of solvation in water, solubility in water, thrombin/trypsin/factor Xa activity, blood-brain barrier permeability, and compound classification. The wide applicability of these descriptors suggests uses in QSAR/QSPR, combinatorial library design, and molecular diversity work.},
  keywords = {molecular descriptors,molecular diversity,QSAR}
}

@misc{landrumRDKitOpensourceCheminformatics2024,
  title = {{{RDKit}}: {{Open-source}} Cheminformatics.},
  shorttitle = {Rdkit/Rdkit},
  author = {Landrum, Greg and Tosco, Paolo and Kelley, Brian and Rodriguez, Ricardo and Cosgrove, David and Vianello, Riccardo and {sriniker} and {gedeck} and Jones, Gareth and NadineSchneider and Kawashima, Eisuke and Nealschneider, Dan and Dalke, Andrew and Swain, Matt and Cole, Brian and Turk, Samo and Savelev, Aleksandr and Vaucher, Alain and W{\'o}jcikowski, Maciej and Take, Ichiru and Scalfani, Vincent F. and Walker, Rachel and Ujihara, Kazuya and Probst, Daniel and {godin}, guillaume and Pahl, Axel and Lehtivarjo, Juuso and Berenger, Francois and {jasondbiggs} and {strets123}},
  year = {2024},
  month = may,
  doi = {10.5281/zenodo.11102446},
  urldate = {2024-05-06},
  abstract = {Release\_2024.03.2 (Changes relative to Release\_2024.03.1) Acknowledgements (Note: I'm no longer attempting to manually curate names. If you would like to see your contribution acknowledged with your name, please set your name in GitHub) Christoph Berg, Anna Br{\"u}nisholz, Michael Cho, David Cosgrove, Andrew Dalke, Peter Eastman, Tad Hurst, Gareth Jones, Brian Kelley, Daniel Levine, Yakov Pechersky, Ricardo Rodriguez, Matt Swain, Paolo Tosco, Riccardo Vianello Backwards incompatible changes The SMARTS for the unbranched alkanes in the fragment descriptors has been corrected. This descriptor will now frequently return different results. New Features and Enhancements: Added JSON parameters to MinimalLib get\_(cx)?sm(ile{\textbar}art)s() functions (github pull \#7194 from ptosco) Include macrocycles in atropisomer calculation by not sanitizing them away (github pull \#7291 from pechersky) C\# Build Net6 library and tests using cmake (github pull \#7326 from jones-gareth) Add option for RASCAL to restrict atom matching to atoms of same degree (github pull \#7344 from DavidACosgrove) Add MolStandardize to C\# wrappers (github pull \#7351 from jones-gareth) CDXML parser doesn't recognize any bonds (github issue \#7357 from bp-kelley) Allow reapplyMolBlockWedging() to restore the original wedging regardless the bond type (github pull \#7386 from rvianello) Add option for non-isomeric SMILES creation in the PostgreSQL cartridge (github pull \#7395 from rvianello) Bug Fixes: Correct unbranched alkane SMARTS to match the description given (github  \#7255 from levineds) restrict the application of 1,3- 1,5- conjugated cation normalization (github pull \#7287 from rvianello) DetermineBondOrders() does not assign single bonds correctly (github issue \#7299 from peastman) re-enable yaehmop support in DetermineBonds (github pull \#7316 from greglandrum) AtomPairs.Utils.NumPiElectrons fails on atoms with dative bonds (github issue \#7318 from ricrogz) Wedge bond from atrop error (github pull \#7321 from tadhurst-cdd) Remove misleading walrus operators (github pull \#7323 from mcs07) SaltRemover may clear computed properties even if no atoms are removed (github issue \#7327 from ricrogz) DetermineBondOrders() makes incorrect assumptions about valence (github issue \#7331 from peastman) remove some warnings with -Wextra (github pull \#7339 from greglandrum) Fixes problem from discussion 7317 (github pull \#7345 from DavidACosgrove) Trigonal Pyramid Carbon may or not have a parity depending on atom ordering (github issue \#7346 from ricrogz) fixes bug with overly large count\_bounds (github pull \#7368 from greglandrum) Fix the Uncharger 'force' option w/ non-neutralizable negatively charged sites (github pull \#7382 from rvianello) Do not apply the normalization of conjugated cations to the oxime oxygen (github pull \#7403 from rvianello) Cleanup work: Code/PgSQL: Fix Pointer vs Datum (Compatibility with PG16) (github pull \#6733 from df7cb) switch to range-based for loops (github pull \#7278 from AnnaBruenisholz) Cleaner forloops, deleting of empty header file (github pull \#7320 from AnnaBruenisholz) Make ctest run installed tests if RDK\_INSTALL\_PYTHON\_TESTS (github pull \#7325 from mcs07) cleanup RDKit::MolOps::detectBondStereochemistry (github pull \#7329 from rvianello) Cleanup of Code/DataStructs (github pull \#7365 from AnnaBruenisholz) Fixes \#7378, raw docstring to escape null chars (github pull \#7379 from pechersky) Include header for boost::numeric\_cast (github pull \#7389 from cho-m)},
  howpublished = {Zenodo}
}

@misc{liawTuneResearchPlatform2018,
  title = {Tune: {{A Research Platform}} for {{Distributed Model Selection}} and {{Training}}},
  shorttitle = {Tune},
  author = {Liaw, Richard and Liang, Eric and Nishihara, Robert and Moritz, Philipp and Gonzalez, Joseph E. and Stoica, Ion},
  year = {2018},
  month = jul,
  number = {arXiv:1807.05118},
  eprint = {1807.05118},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1807.05118},
  urldate = {2024-05-06},
  abstract = {Modern machine learning algorithms are increasingly computationally demanding, requiring specialized hardware and distributed computation to achieve high performance in a reasonable time frame. Many hyperparameter search algorithms have been proposed for improving the efficiency of model selection, however their adaptation to the distributed compute environment is often ad-hoc. We propose Tune, a unified framework for model selection and training that provides a narrow-waist interface between training scripts and search algorithms. We show that this interface meets the requirements for a broad range of hyperparameter search algorithms, allows straightforward scaling of search to large clusters, and simplifies algorithm implementation. We demonstrate the implementation of several state-of-the-art hyperparameter search algorithms in Tune. Tune is available at http://ray.readthedocs.io/en/latest/tune.html.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Distributed Parallel and Cluster Computing,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/wudw/Zotero/storage/URU9L5GW/Liaw et al. - 2018 - Tune A Research Platform for Distributed Model Se.pdf}
}

@misc{liHyperbandNovelBanditBased2018,
  title = {Hyperband: {{A Novel Bandit-Based Approach}} to {{Hyperparameter Optimization}}},
  shorttitle = {Hyperband},
  author = {Li, Lisha and Jamieson, Kevin and DeSalvo, Giulia and Rostamizadeh, Afshin and Talwalkar, Ameet},
  year = {2018},
  month = jun,
  number = {arXiv:1603.06560},
  eprint = {1603.06560},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1603.06560},
  urldate = {2024-05-06},
  abstract = {Performance of machine learning algorithms depends critically on identifying a good set of hyperparameters. While recent approaches use Bayesian optimization to adaptively select configurations, we focus on speeding up random search through adaptive resource allocation and early-stopping. We formulate hyperparameter optimization as a pure-exploration non-stochastic infinite-armed bandit problem where a predefined resource like iterations, data samples, or features is allocated to randomly sampled configurations. We introduce a novel algorithm, Hyperband, for this framework and analyze its theoretical properties, providing several desirable guarantees. Furthermore, we compare Hyperband with popular Bayesian optimization methods on a suite of hyperparameter optimization problems. We observe that Hyperband can provide over an order-of-magnitude speedup over our competitor set on a variety of deep-learning and kernel-based learning problems.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/wudw/Zotero/storage/CN74FKCW/Li et al. - 2018 - Hyperband A Novel Bandit-Based Approach to Hyperp.pdf}
}

@article{limDelfosDeepLearning2019,
  title = {Delfos: Deep Learning Model for Prediction of Solvation Free Energies in Generic Organic Solvents},
  shorttitle = {Delfos},
  author = {Lim, Hyuntae and Jung, YounJoon},
  year = {2019},
  month = sep,
  journal = {Chemical Science},
  volume = {10},
  number = {36},
  pages = {8306--8315},
  publisher = {The Royal Society of Chemistry},
  issn = {2041-6539},
  doi = {10.1039/C9SC02452B},
  urldate = {2024-05-06},
  abstract = {Prediction of aqueous solubilities or hydration free energies is an extensively studied area in machine learning applications in chemistry since water is the sole solvent in the living system. However, for non-aqueous solutions, few machine learning studies have been undertaken so far despite the fact that the solvation mechanism plays an important role in various chemical reactions. Here, we introduce Delfos (deep learning model for solvation free energies in generic organic solvents), which is a novel, machine-learning-based QSPR method which predicts solvation free energies for various organic solute and solvent systems. A novelty of Delfos involves two separate solvent and solute encoder networks that can quantify structural features of given compounds via word embedding and recurrent layers, augmented with the attention mechanism which extracts important substructures from outputs of recurrent neural networks. As a result, the predictor network calculates the solvation free energy of a given solvent--solute pair using features from encoders. With the results obtained from extensive calculations using 2495 solute--solvent pairs, we demonstrate that Delfos not only has great potential in showing accuracy comparable to that of the state-of-the-art computational chemistry methods, but also offers information about which substructures play a dominant role in the solvation process.},
  langid = {english},
  file = {/Users/wudw/Zotero/storage/49GFMJVV/Lim and Jung - 2019 - Delfos deep learning model for prediction of solv.pdf;/Users/wudw/Zotero/storage/ZULC5DPU/Lim and Jung - 2019 - Delfos deep learning model for prediction of solv.pdf}
}

@misc{paszkePyTorchImperativeStyle2019,
  title = {{{PyTorch}}: {{An Imperative Style}}, {{High-Performance Deep Learning Library}}},
  shorttitle = {{{PyTorch}}},
  author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and K{\"o}pf, Andreas and Yang, Edward and DeVito, Zach and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  year = {2019},
  month = dec,
  number = {arXiv:1912.01703},
  eprint = {1912.01703},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1912.01703},
  urldate = {2024-05-06},
  abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Mathematical Software,Statistics - Machine Learning},
  file = {/Users/wudw/Zotero/storage/JG3PQS7K/Paszke et al. - 2019 - PyTorch An Imperative Style, High-Performance Dee.pdf}
}

@article{pedregosaScikitlearnMachineLearning2011,
  title = {Scikit-Learn: {{Machine Learning}} in {{Python}}},
  shorttitle = {Scikit-Learn},
  author = {Pedregosa, Fabian and Varoquaux, Ga{\"e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, {\'E}douard},
  year = {2011},
  journal = {Journal of Machine Learning Research},
  volume = {12},
  number = {85},
  pages = {2825--2830},
  issn = {1533-7928},
  urldate = {2024-05-06},
  abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
  file = {/Users/wudw/Zotero/storage/PYMRS87G/Pedregosa et al. - 2011 - Scikit-learn Machine Learning in Python.pdf}
}

@article{ritchieImpactAromaticRing2011,
  title = {The Impact of Aromatic Ring Count on Compound Developability: Further Insights by Examining Carbo- and Hetero-Aromatic and -Aliphatic Ring Types},
  shorttitle = {The Impact of Aromatic Ring Count on Compound Developability},
  author = {Ritchie, Timothy J. and Macdonald, Simon J. F. and Young, Robert J. and Pickett, Stephen D.},
  year = {2011},
  month = feb,
  journal = {Drug Discovery Today},
  volume = {16},
  number = {3},
  pages = {164--171},
  issn = {1359-6446},
  doi = {10.1016/j.drudis.2010.11.014},
  urldate = {2024-05-06},
  abstract = {The impact of carboaromatic, heteroaromatic, carboaliphatic and heteroaliphatic ring counts and fused aromatic ring count on several developability measures (solubility, lipophilicity, protein binding, P450 inhibition and hERG binding) is the topic for this review article. Recent results indicate that increasing ring counts have detrimental effects on developability in the order carboaromatics{$\gg$}heteroaromatics{$>$}carboaliphatics{$>$}heteroaliphatics, with heteroaliphatics exerting a beneficial effect in many cases. Increasing aromatic ring count exerts effects on several developability parameters that are lipophilicity- and size-independent, and fused aromatic systems have a beneficial effect relative to their nonfused counterparts. Increasing aromatic ring count has a detrimental effect on human bioavailability parameters, and heteroaromatic ring count (but not other ring counts) has increased over time in marketed oral drugs.}
}

@article{rogersExtendedConnectivityFingerprints2010,
  title = {Extended-{{Connectivity Fingerprints}}},
  author = {Rogers, David and Hahn, Mathew},
  year = {2010},
  month = may,
  journal = {Journal of Chemical Information and Modeling},
  volume = {50},
  number = {5},
  pages = {742--754},
  publisher = {American Chemical Society},
  issn = {1549-9596},
  doi = {10.1021/ci100050t},
  urldate = {2024-05-06},
  abstract = {Extended-connectivity fingerprints (ECFPs) are a novel class of topological fingerprints for molecular characterization. Historically, topological fingerprints were developed for substructure and similarity searching. ECFPs were developed specifically for structure-activity modeling. ECFPs are circular fingerprints with a number of useful qualities: they can be very rapidly calculated; they are not predefined and can represent an essentially infinite number of different molecular features (including stereochemical information); their features represent the presence of particular substructures, allowing easier interpretation of analysis results; and the ECFP algorithm can be tailored to generate different types of circular fingerprints, optimized for different uses. While the use of ECFPs has been widely adopted and validated, a description of their implementation has not previously been presented in the literature.},
  file = {/Users/wudw/Zotero/storage/D43V9DTG/Rogers and Hahn - 2010 - Extended-Connectivity Fingerprints.pdf}
}

@misc{vaswaniAttentionAllYou2023a,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2023},
  month = aug,
  number = {arXiv:1706.03762},
  eprint = {1706.03762},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-05-06},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/wudw/Zotero/storage/7WQ2G8X9/Vaswani et al. - 2023 - Attention Is All You Need.pdf}
}

@misc{wagerDropoutTrainingAdaptive2013,
  title = {Dropout {{Training}} as {{Adaptive Regularization}}},
  author = {Wager, Stefan and Wang, Sida and Liang, Percy},
  year = {2013},
  month = nov,
  number = {arXiv:1307.1493},
  eprint = {1307.1493},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1307.1493},
  urldate = {2024-05-06},
  abstract = {Dropout and other feature noising schemes control overfitting by artificially corrupting the training data. For generalized linear models, dropout performs a form of adaptive regularization. Using this viewpoint, we show that the dropout regularizer is first-order equivalent to an L2 regularizer applied after scaling the features by an estimate of the inverse diagonal Fisher information matrix. We also establish a connection to AdaGrad, an online learning algorithm, and find that a close relative of AdaGrad operates by repeatedly solving linear dropout-regularized problems. By casting dropout as regularization, we develop a natural semi-supervised algorithm that uses unlabeled data to create a better adaptive regularizer. We apply this idea to document classification tasks, and show that it consistently boosts the performance of dropout training, improving on state-of-the-art results on the IMDB reviews dataset.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Statistics - Methodology},
  file = {/Users/wudw/Zotero/storage/CURMKULK/Wager et al. - 2013 - Dropout Training as Adaptive Regularization.pdf}
}

@article{wildmanPredictionPhysicochemicalParameters1999,
  title = {Prediction of {{Physicochemical Parameters}} by {{Atomic Contributions}}},
  author = {Wildman, Scott A. and Crippen, Gordon M.},
  year = {1999},
  month = sep,
  journal = {Journal of Chemical Information and Computer Sciences},
  volume = {39},
  number = {5},
  pages = {868--873},
  publisher = {American Chemical Society},
  issn = {0095-2338},
  doi = {10.1021/ci990307l},
  urldate = {2024-05-06},
  abstract = {We present a new atom type classification system for use in atom-based calculation of partition coefficient (log P) and molar refractivity (MR) designed in part to address published concerns of previous atomic methods. The 68 atomic contributions to log P have been determined by fitting an extensive training set of 9920 molecules, with r2 = 0.918 and {$\sigma$} = 0.677. A separate set of 3412 molecules was used for the determination of contributions to MR with r2 = 0.997 and {$\sigma$} = 1.43. Both calculations are shown to have high predictive ability.},
  file = {/Users/wudw/Zotero/storage/U6SNUS4W/Wildman and Crippen - 1999 - Prediction of Physicochemical Parameters by Atomic.pdf}
}

@article{yangAnalyzingLearnedMolecular2019,
  title = {Analyzing {{Learned Molecular Representations}} for {{Property Prediction}}},
  author = {Yang, Kevin and Swanson, Kyle and Jin, Wengong and Coley, Connor and Eiden, Philipp and Gao, Hua and {Guzman-Perez}, Angel and Hopper, Timothy and Kelley, Brian and Mathea, Miriam and Palmer, Andrew and Settels, Volker and Jaakkola, Tommi and Jensen, Klavs and Barzilay, Regina},
  year = {2019},
  month = aug,
  journal = {Journal of Chemical Information and Modeling},
  volume = {59},
  number = {8},
  pages = {3370--3388},
  publisher = {American Chemical Society},
  issn = {1549-9596},
  doi = {10.1021/acs.jcim.9b00237},
  urldate = {2024-05-06},
  abstract = {Advancements in neural machinery have led to a wide range of algorithmic solutions for molecular property prediction. Two classes of models in particular have yielded promising results: neural networks applied to computed molecular fingerprints or expert-crafted descriptors and graph convolutional neural networks that construct a learned molecular representation by operating on the graph structure of the molecule. However, recent literature has yet to clearly determine which of these two methods is superior when generalizing to new chemical space. Furthermore, prior research has rarely examined these new models in industry research settings in comparison to existing employed models. In this paper, we benchmark models extensively on 19 public and 16 proprietary industrial data sets spanning a wide variety of chemical end points. In addition, we introduce a graph convolutional model that consistently matches or outperforms models using fixed molecular descriptors as well as previous graph neural architectures on both public and proprietary data sets. Our empirical findings indicate that while approaches based on these representations have yet to reach the level of experimental reproducibility, our proposed model nevertheless offers significant improvements over models currently used in industrial workflows.},
  file = {/Users/wudw/Zotero/storage/2KPX7Q89/Yang et al. - 2019 - Analyzing Learned Molecular Representations for Pr.pdf}
}
